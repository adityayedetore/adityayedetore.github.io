@inproceedings{yedetore2023stimulus,
  pdf = {povstim_with_childes_acl_2023.pdf},
  poster = {poster-final.pdf},
  abbr = {ACL},
  arxiv = {2301.11462},
  title = "How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech",
  author = "Yedetore, Aditya  and
    Linzen, Tal  and
    Frank, Robert  and
    McCoy, R. Thomas",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = jul,
  year = "2023",
  address = "Toronto, Canada",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.acl-long.521",
  doi = "10.18653/v1/2023.acl-long.521",
  pages = "9370--9393",
  abstract = "When acquiring syntax, children consistently choose hierarchical rules over competing non-hierarchical possibilities. Is this preference due to a learning bias for hierarchical structure, or due to more general biases that interact with hierarchical cues in children{'}s linguistic input? We explore these possibilities by training LSTMs and Transformers - two types of neural networks without a hierarchical bias - on data similar in quantity and content to children{'}s linguistic input: text from the CHILDES corpus. We then evaluate what these models have learned about English yes/no questions, a phenomenon for which hierarchical structure is crucial. We find that, though they perform well at capturing the surface statistics of child-directed speech (as measured by perplexity), both model types generalize in a way more consistent with an incorrect linear rule than the correct hierarchical rule. These results suggest that human-like generalization from text alone requires stronger biases than the general sequence-processing biases of standard neural network architectures.",
  selected={true}
}
