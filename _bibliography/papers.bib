@misc{yedetore2023stimulus,
  doi = {10.48550/ARXIV.2301.11462},
  url = {https://arxiv.org/abs/2301.11462},
  pdf = {povstim_with_childes_acl_2023.pdf},
  poster = {poster-final.pdf},
  abbr = {NAACL},
  arxiv = {2301.11462},
  author = {Yedetore, Aditya and Linzen, Tal and Frank, Robert and McCoy, R. Thomas},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences, J.4; I.2.7},
  title = {How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license},
  selected={false}
}

@inproceedings{yedetore2023stimulus,
  doi = {10.48550/ARXIV.2301.11462},
  url = {https://aclanthology.org/2023.acl-long.521/},
  pdf = {povstim_with_childes_acl_2023.pdf},
  poster = {poster-final.pdf},
  abbr = {NAACL},
  title = "How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech",
  author = "Yedetore, Aditya  and
    Linzen, Tal  and
    Frank, Robert  and
    McCoy, R. Thomas",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = jul,
  year = "2023",
  address = "Toronto, Canada",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.acl-long.521",
  doi = "10.18653/v1/2023.acl-long.521",
  pages = "9370--9393",
  abstract = "When acquiring syntax, children consistently choose hierarchical rules over competing non-hierarchical possibilities. Is this preference due to a learning bias for hierarchical structure, or due to more general biases that interact with hierarchical cues in children{'}s linguistic input? We explore these possibilities by training LSTMs and Transformers - two types of neural networks without a hierarchical bias - on data similar in quantity and content to children{'}s linguistic input: text from the CHILDES corpus. We then evaluate what these models have learned about English yes/no questions, a phenomenon for which hierarchical structure is crucial. We find that, though they perform well at capturing the surface statistics of child-directed speech (as measured by perplexity), both model types generalize in a way more consistent with an incorrect linear rule than the correct hierarchical rule. These results suggest that human-like generalization from text alone requires stronger biases than the general sequence-processing biases of standard neural network architectures.",
  selected={true}
}
